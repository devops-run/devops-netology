# Домашнее задание к занятию "6.6. Troubleshooting"

## Задача 1

Перед выполнением задания ознакомьтесь с документацией по [администрированию MongoDB](https://docs.mongodb.com/manual/administration/).

Пользователь (разработчик) написал в канал поддержки, что у него уже 3 минуты происходит CRUD операция в MongoDB и её 
нужно прервать. 

Вы как инженер поддержки решили произвести данную операцию:
- напишите список операций, которые вы будете производить для остановки запроса пользователя
- предложите вариант решения проблемы с долгими (зависающими) запросами в MongoDB

### Решение.

- Cписок операций для остановки запроса пользователя.        

1. Поиск и остановка запроса:      

Определяем список запросов, выполняющихся больше 3-х минут:     
```
db.currentOp({"secs_running":{$gte: 180}})  

```

2. Определяем необходимый opid и прерываем его:     
```
db.killOp(opid) 
```
- Вариант решения проблемы с долгими/зависающими запросами в MongoDB:

Решение - использовать профилировщик базы данных.     
Профилировщик базы данных собирает детализированные данные о запросах MongoDB дольше, чем определенный порог — порог в миллисекундах, при котором профилировщик базы данных считает запрос медленным. Профилировщик базы данных записывает все собранные данные в system.profileколлекцию, чтобы мы могли проанализировать их позже.    
    
Также можно установить некоторое большее значение в качестве порога, чтобы минимизировать количество запросов для анализа.      
Например, следующая команда устанавливает уровень профилирования для текущей базы данных равным 1, а порог медленной работы равен 1000 миллисекунд:
```
user:PRIMARY> db.setProfilingLevel(1, 1000)
{ “was” : 0, “slowms” : 100, “ok” : 1 }
user:PRIMARY> db.getProfilingStatus()
{ “was” : 1, “slowms” : 1000 }
```

База данных будет регистрировать операции медленнее, чем 1000 миллисекунд в system.profileсборе.        
    
Теперь можно запрашивать данные по этой коллекции и анализировать:      
```
db.system.profile.find().pretty()
// or get 'query' operations only and specified fields
db.system.profile.find( { op: { $eq : 'query' } } , {"millis": 1, "ns": 1, "ts": 1,"query": 1}).sort( { ts : -1 } ).pretty()
```
Для решения использовал официальную документацию и [данную статью](https://medium.com/mongodb-cowboys/troubleshooting-mongodb-100-cpu-load-and-slow-queries-da622c6e1339).

## Задача 2

Перед выполнением задания познакомьтесь с документацией по [Redis latency troobleshooting](https://redis.io/topics/latency).

Вы запустили инстанс Redis для использования совместно с сервисом, который использует механизм TTL. 
Причем отношение количества записанных key-value значений к количеству истёкших значений есть величина постоянная и
увеличивается пропорционально количеству реплик сервиса. 

При масштабировании сервиса до N реплик вы увидели, что:
- сначала рост отношения записанных значений к истекшим
- Redis блокирует операции записи

Как вы думаете, в чем может быть проблема?
 
#### Решение

Рост “новых” пар ключ-значение по отношению к тем, которые должны быть очищены говорит о том, что растет размер хранилища, а значит ресурс памяти заканчивается. Блокировка на запись, скорее всего, говорит о том, то выделенная память закончилась. 
    
Решением, на мой взгляд, может быть настройка параметров отвечающих за "Распределение памяти".

Maxmemory configuration directive

For example, to configure a memory limit of 100 megabytes, you can use the following directive inside the redis.conf file:

maxmemory 100mb  


Если maxmemory параметр не установлен, Redis будет продолжать выделять память по своему усмотрению и, таким образом, может (постепенно) поглотить всю свободную память. Поэтому обычно рекомендуется настроить некоторый лимит. Вы также можете установить maxmemory-policy значение noeviction (которое не является значением по умолчанию в некоторых старых версиях Redis).

(для ответа использовал материалы с  Redis latency troobleshooting, на практике не проверял)    

Когда у редиса начинается этот механизм? - когда происходит рост коэффициента фрагментации памяти. 

### #########Доработка################
Использовал пример из жизни (чужой):
https://highload.today/blogs/redis-bolshoe-potreblenie-ram-i-pri-chem-tut-ttl/

Судя по всему, в нашем случае происходит заполнение памяти уже просроченнывми данными, которые зря занимают место.
И т.к количество истёкших значений постоянно увеличивается, то настройка параметра time-to-live (время жизни) должно решить проблему.
Память будет автоматически освобождаться.

#### Команды для настройки: 
There are four ways to set the expiration time in Redis:    
expire key TTL(seconds): Set the key to expire after n seconds;     
pexpire key TTL(milliseconds): Set the key to expire after n milliseconds;      
expireat key timestamp: Set the key to expire after a specific timestamp (accurate to seconds);     
pexpireat key millisecondsTimestamp: Set the key to expire after a specific timestamp (accurate to milliseconds);       
    
The time complexity is all O(1), and usages of them are similar.        

"OK"
redis> EXPIRE mykey 10
(integer) 1
redis> TTL mykey
(integer) 10
redis> SET mykey "Hello World"
"OK"
redis> TTL mykey
(integer) -1
redis> 
### #########Доработка 2################
Задержка, генерируемая по истечении срока действия
Redis удаляет ключи с истекшим сроком действия двумя способами:

Одним из ленивых способов истекает срок действия ключа, когда он запрашивается командой, но оказывается, что срок его действия уже истек.
У одного активного способа истекает срок действия нескольких ключей каждые 100 миллисекунд.
Активный истекающий предназначен для адаптации. Цикл истечения срока действия запускается каждые 100 миллисекунд (10 раз в секунду) и выполняет следующие действия:

Образцы ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOPключей, удаление всех ключей, срок действия которых уже истек.
Если было обнаружено, что более 25% ключей истекли, повторите.
Учитывая, что ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOPпо умолчанию установлено значение 20, а процесс выполняется десять раз в секунду, обычно активно истекает только 200 ключей в секунду. Этого достаточно, чтобы очистить базу данных достаточно быстро, даже если доступ к ключам с истекшим сроком действия не осуществляется в течение длительного времени, так что ленивый алгоритм не помогает. В то же время истечение срока действия всего 200 ключей в секунду не влияет на задержку экземпляра Redis.

Однако алгоритм является адаптивным и будет зацикливаться, если обнаружит, что более 25% ключей уже истекли в наборе выбранных ключей. Но, учитывая, что мы запускаем алгоритм десять раз в секунду, это означает, что неудачное событие для более чем 25% ключей в нашей случайной выборке истекает, по крайней мере, в ту же секунду.

***По сути, это означает, что если в базе данных много-много ключей, срок действия которых истекает в одну и ту же секунду, и они составляют не менее 25% от текущего набора ключей с истекшим сроком действия, Redis может заблокировать, чтобы процент ключей, срок действия которых уже истек, был ниже 25%.***

Этот подход необходим для того, чтобы избежать использования слишком большого объема памяти для ключей, срок действия которых уже истек, и обычно он абсолютно безвреден, поскольку странно, что срок действия большого количества ключей истекает в одну и ту же секунду, но не исключено, что пользователь EXPIREATактивно использовал одно и то же время в Unix.

Вкратце: имейте в виду, что многие ключи, истекающие в один и тот же момент, могут быть источником задержки.
## Задача 3

Перед выполнением задания познакомьтесь с документацией по [Common Mysql errors](https://dev.mysql.com/doc/refman/8.0/en/common-errors.html).

Вы подняли базу данных MySQL для использования в гис-системе. При росте количества записей, в таблицах базы,
пользователи начали жаловаться на ошибки вида:
```python
InterfaceError: (InterfaceError) 2013: Lost connection to MySQL server during query u'SELECT..... '
```

Как вы думаете, почему это начало происходить и как локализовать проблему?

Какие пути решения данной проблемы вы можете предложить?

#### Решение

Чаще всего данная ситуация возникает, когда происходит выборка очень большого количества данных, т.к. проблема стала возникать не сразу, а при росте количества записей.
В качестве решения можно предложить:
   1. Увеличить значение параметров : connect_timeout, interactive_timeout, wait_timeout    
   2. Добавить ресурсов на машине   
   3. Создать индексы для оптимизации  и ускорения запросов (определить по плану запросов)  
    
Документация рекомендует нам увеличить настройку net_read_timeout с дефолтных 30 до 60 секунд, но делать это стоит только после того, как закончились пути оптимизация запроса с помощью индексов и других средств БД.


Так же могут быть сбои на сетевой инфраструктуре, в таком случае необходимо увеличивать net_read_timeout (https://dev.mysql.com/doc/refman/5.7/en/error-lost-connection.html).
Как дополнительный вариант можно расширить максимальное число соединений :  max_connections (https://www.opennet.ru/docs/RUS/sql_error/chap10.html).

## Задача 4

Перед выполнением задания ознакомтесь со статьей [Common PostgreSQL errors](https://www.percona.com/blog/2020/06/05/10-common-postgresql-errors/) из блога Percona.

Вы решили перевести гис-систему из задачи 3 на PostgreSQL, так как прочитали в документации, что эта СУБД работает с 
большим объемом данных лучше, чем MySQL.

После запуска пользователи начали жаловаться, что СУБД время от времени становится недоступной. В dmesg вы видите, что:

`postmaster invoked oom-killer`

Как вы думаете, что происходит?

Как бы вы решили данную проблему?

#### Решение
Судя по всему oom-killer  завершает процесс.    
Чаще всего это происходит т.к  заканчивается пространство на диске или память.       
Если закончилось пространство на диске, выход один — освободить место и перезапустить базу данных.  
    
Если проблема в нехватке RAM, то очевидное решение -  Добавить память, + тонко настроить базу на использование RAM. (wal_buffers, max_connections, shared_buffer, work_mem, effective_cache_size, maintenance_work_mem)         

Также можно установить для vm.overcommit_memory значение 2.     

Это не гарантирует, что OOM-Killer не придется вмешиваться, но снизит вероятность принудительного завершения процесса PostgreSQL.   
    
А что если у нас менеджед решение и доступа на виртушки у нас нет? -    
    
Тогда используем параметры, управляющие самим OOM-Killer.   
    
чтобы OOM-Killer не завершил процесс             
    
sudo -s sysctl -w vm.oom-kill = 0   
